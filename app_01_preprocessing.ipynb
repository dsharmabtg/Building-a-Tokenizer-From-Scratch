{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer\n",
        "!pip install pdfminer.six\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arrd1BX55atv",
        "outputId": "6776c8cd-0af2-4420-cc63-192d6b84c19f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer\n",
            "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome (from pdfminer)\n",
            "  Downloading pycryptodome-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pdfminer\n",
            "  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140084 sha256=89e8bff7a21641588fd713dc4268a54fe5bf315da60d3e7bec0c5f4ffabf1f2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/c1/68/f7bd0a8f514661f76b5cbe3b5f76e0033d79f1296012cbbf72\n",
            "Successfully built pdfminer\n",
            "Installing collected packages: pycryptodome, pdfminer\n",
            "Successfully installed pdfminer-20191125 pycryptodome-3.19.0\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20221105\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOF4ESqK5SgN"
      },
      "source": [
        "# 1. CONVERT PDF TO TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zugAxyQB5SgV"
      },
      "outputs": [],
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "import re\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeDZT0f95SgZ",
        "outputId": "daae18f7-e834-45a4-99d6-9e9c4cbc52a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'the_return_of_sherlock_holmes.pdf' successfully copied to 'the_return_of_sherlock_holmes'\n"
          ]
        }
      ],
      "source": [
        "#/content/the_return_of_sherlock_holmes.pdf\n",
        "doc_name = \"the_return_of_sherlock_holmes\"\n",
        "\n",
        "process_folder = doc_name\n",
        "if not os.path.exists(process_folder):\n",
        "    os.makedirs(process_folder)\n",
        "\n",
        "path = process_folder + \"/\" + doc_name + \".pdf\"\n",
        "text_filename = process_folder + \"/\" + doc_name + \".txt\"\n",
        "\n",
        "# Attempt to copy the PDF\n",
        "try:\n",
        "    shutil.copy(f\"{doc_name}.pdf\", process_folder)\n",
        "    print(f\"File '{doc_name}.pdf' successfully copied to '{process_folder}'\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{doc_name}.pdf' not found. Please download it from the Resources Area.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij5wGal05Sga"
      },
      "source": [
        "### Save the entire text to a text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WhRyDNdu5Sga"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "document = PdfReader(open(path, \"rb\"))  # replace with your PDF file\n",
        "\n",
        "\n",
        "all_pages_text = []\n",
        "page_num = 0\n",
        "\n",
        "for i in range(len(document.pages)):\n",
        "    # Convert page to PDF File Writer object\n",
        "    page = document.pages[i]\n",
        "\n",
        "    # Extract text from page\n",
        "    page_text = page.extract_text()\n",
        "\n",
        "    start_page = i - 1\n",
        "\n",
        "    all_pages_text.append(page_text)\n",
        "\n",
        "with open(text_filename, 'w', encoding=\"utf-8\") as file:\n",
        "    for item in all_pages_text:\n",
        "        file.write(str(item) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR5nH-Dz5Sgb"
      },
      "source": [
        "### Split text by sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54cjk_wf5Sgb",
        "outputId": "a121a8dc-7bc6-413b-ad76-56af4f2330dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "# Download the punkt tokenizer models\n",
        "nltk.download('punkt')\n",
        "\n",
        "def file_to_sentences(filepath):\n",
        "    \"\"\"\n",
        "    Given a filepath, read the text file and split it into sentences.\n",
        "    Returns a list of sentences.\n",
        "    \"\"\"\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "        sentences = nltk.tokenize.sent_tokenize(content)\n",
        "    return sentences\n",
        "\n",
        "def add_sentences_to_dataframe(sentences, dataframe):\n",
        "    \"\"\"\n",
        "    Add sentences to the given dataframe.\n",
        "    \"\"\"\n",
        "    for sentence in sentences:\n",
        "        dataframe = dataframe.append({'Sentence': sentence}, ignore_index=True)\n",
        "    return dataframe\n",
        "\n",
        "# Initialize an empty DataFrame with one column \"Sentence\"\n",
        "df = pd.DataFrame(columns=[\"text\"])\n",
        "\n",
        "# Initialize a list to hold all sentences\n",
        "all_sentences = []\n",
        "\n",
        "# Set the directory where the text files are located\n",
        "filepath = f\"{doc_name}/{doc_name}.txt\"\n",
        "all_sentences.extend(file_to_sentences(filepath))\n",
        "\n",
        "\n",
        "# Convert the list of sentences to a DataFrame\n",
        "df = pd.DataFrame(all_sentences, columns=[\"Sentence\"])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(f\"{doc_name}_sentences.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mQxsCgE5Sgc"
      },
      "source": [
        "# 2. PRE-PROCESS THE TEXT\n",
        "\n",
        " - Load as unicode (which is usually the default of Python anyways)\n",
        " - Lowercase the entire text\n",
        " - Exclude all symbols except these: . , ; ? ! ' \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH1K7vuz5Sgc",
        "outputId": "2b7b0e62-a446-4b86-9115-0b070c5bb3ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text has been cleaned and saved to the_return_of_sherlock_holmes/the_return_of_sherlock_holmes_clean.txt.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# Function to convert to ASCII\n",
        "def to_ascii(text):\n",
        "    normalized = unicodedata.normalize('NFKD', text)\n",
        "    return normalized.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "# Function to clean the text\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase and to ASCII\n",
        "    text = to_ascii(text.lower())\n",
        "    # Keep only alphabetic characters and spaces\n",
        "    text = re.sub(r'[^a-z\\s]+', ' ', text)\n",
        "    # Normalize spaces to a single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Read the content of the file\n",
        "filepath = f\"{doc_name}/{doc_name}.txt\"\n",
        "\n",
        "with open(filepath, 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Clean the content\n",
        "cleaned_content = clean_text(content)\n",
        "\n",
        "# Save the cleaned content back to a file\n",
        "filepath_clean = f\"{doc_name}/{doc_name}_clean.txt\"\n",
        "with open(filepath_clean, 'w', encoding='ascii') as file:\n",
        "    file.write(cleaned_content)\n",
        "\n",
        "print(f\"The text has been cleaned and saved to {filepath_clean}.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EERHQ0H5Sgd"
      },
      "source": [
        "# 3. TOKENIZE THE TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FinX4O45Sge",
        "outputId": "1b45f6ea-edae-4893-8ea4-e53d5fa64047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique words: 9088\n",
            "The list of unique words has been saved to the_return_of_sherlock_holmes/the_return_of_sherlock_holmes_words.txt\n"
          ]
        }
      ],
      "source": [
        "# Function to extract unique words from the text\n",
        "def extract_unique_words(text):\n",
        "    # Split text into words based on whitespace\n",
        "    words = text.split()\n",
        "    # Use a set to remove duplicates, extracting the unique words\n",
        "    unique_words = set(words)\n",
        "    return unique_words\n",
        "\n",
        "# Read the cleaned content of the file\n",
        "filepath_clean = f\"{doc_name}/{doc_name}_clean.txt\"\n",
        "with open(filepath_clean, 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Get the list of unique words\n",
        "unique_words_set = extract_unique_words(content)\n",
        "unique_words_list = sorted(list(unique_words_set))  # Convert to a sorted list if order is needed\n",
        "\n",
        "# Optionally save the unique words to a file\n",
        "filepath_words = f\"{doc_name}/{doc_name}_words.txt\"\n",
        "with open(filepath_words, 'w', encoding='utf-8') as file:\n",
        "    for word in unique_words_list:\n",
        "        file.write(word + '\\n')\n",
        "\n",
        "print(f\"Total unique words: {len(unique_words_list)}\")\n",
        "print(f\"The list of unique words has been saved to {filepath_words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6Ic7jA35Sge"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}